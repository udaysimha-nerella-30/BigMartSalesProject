{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a12bd89e",
   "metadata": {},
   "source": [
    "# BigMart Sales Prediction - Test Data Production Pipeline\n",
    "\n",
    "This notebook uses the production-ready ensemble pipeline to generate predictions on test data.\n",
    "\n",
    "## Overview\n",
    "- Load the production pipeline and models\n",
    "- Load test data from code folder\n",
    "- Generate predictions using the champion ensemble model\n",
    "- Save predictions for submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c271c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading necessary libraries and production pipeline...\n",
      "Libraries loaded successfully\n",
      "Production pipeline imported successfully\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading necessary libraries and production pipeline...\")\n",
    "\n",
    "# Standard data science libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "# Add the production pipeline to path\n",
    "sys.path.append('../production_models_final')\n",
    "\n",
    "# Import the production pipeline\n",
    "from bigmart_production_pipeline import BigMartProductionPipeline, predict_bigmart_sales\n",
    "\n",
    "print(\"Libraries loaded successfully\")\n",
    "print(\"Production pipeline imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7baf2cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading test data from code folder...\n",
      "Test data loaded successfully\n",
      "Test data shape: (5681, 11)\n",
      "Test data columns: ['Item_Identifier', 'Item_Weight', 'Item_Fat_Content', 'Item_Visibility', 'Item_Type', 'Item_MRP', 'Outlet_Identifier', 'Outlet_Establishment_Year', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']\n",
      "\n",
      "Test data info:\n",
      "Number of samples: 5681\n",
      "Number of features: 11\n",
      "\n",
      "Missing values per column:\n",
      "   Item_Weight: 976 (17.2%)\n",
      "   Outlet_Size: 1606 (28.3%)\n",
      "\n",
      "First 5 rows of test data:\n",
      "  Item_Identifier  Item_Weight Item_Fat_Content  Item_Visibility    Item_Type  \\\n",
      "0           FDW58       20.750          Low Fat         0.007565  Snack Foods   \n",
      "1           FDW14        8.300              reg         0.038428        Dairy   \n",
      "2           NCN55       14.600          Low Fat         0.099575       Others   \n",
      "3           FDQ58        7.315          Low Fat         0.015388  Snack Foods   \n",
      "4           FDY38          NaN          Regular         0.118599        Dairy   \n",
      "\n",
      "   Item_MRP Outlet_Identifier  Outlet_Establishment_Year Outlet_Size  \\\n",
      "0  107.8622            OUT049                       1999      Medium   \n",
      "1   87.3198            OUT017                       2007         NaN   \n",
      "2  241.7538            OUT010                       1998         NaN   \n",
      "3  155.0340            OUT017                       2007         NaN   \n",
      "4  234.2300            OUT027                       1985      Medium   \n",
      "\n",
      "  Outlet_Location_Type        Outlet_Type  \n",
      "0               Tier 1  Supermarket Type1  \n",
      "1               Tier 2  Supermarket Type1  \n",
      "2               Tier 3      Grocery Store  \n",
      "3               Tier 2  Supermarket Type1  \n",
      "4               Tier 3  Supermarket Type3  \n"
     ]
    }
   ],
   "source": [
    "print(\"Loading test data from code folder...\")\n",
    "\n",
    "# Load test data\n",
    "test_data_path = \"../code/test_AbJTz2l.csv\"\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "print(f\"Test data loaded successfully\")\n",
    "print(f\"Test data shape: {test_data.shape}\")\n",
    "print(f\"Test data columns: {test_data.columns.tolist()}\")\n",
    "\n",
    "# Display basic info about test data\n",
    "print(f\"\\nTest data info:\")\n",
    "print(f\"Number of samples: {len(test_data)}\")\n",
    "print(f\"Number of features: {test_data.shape[1]}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nMissing values per column:\")\n",
    "missing_counts = test_data.isnull().sum()\n",
    "for col, count in missing_counts.items():\n",
    "    if count > 0:\n",
    "        print(f\"   {col}: {count} ({count/len(test_data)*100:.1f}%)\")\n",
    "\n",
    "# Display first few rows\n",
    "print(f\"\\nFirst 5 rows of test data:\")\n",
    "print(test_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1c993e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing production pipeline...\n",
      "Using configuration: ensemble_config_20250907_121520.json\n",
      "Models directory: ../production_models_final\n",
      "Loading production pipeline from: ../production_models_final\n",
      "✓ Configuration loaded\n",
      "✓ Preprocessor loaded\n",
      "✓ et_optimized_advanced loaded\n",
      "✓ gb_optimized_advanced loaded\n",
      "✓ xgb_optimized_advanced loaded\n",
      "✓ rf_optimized_advanced loaded\n",
      "✓ Sophisticated ensembles loaded: 11 models\n",
      "✓ Pipeline fully loaded and ready for production!\n",
      "\n",
      "Pipeline Information:\n",
      "   Loaded models: ['et_optimized_advanced', 'gb_optimized_advanced', 'xgb_optimized_advanced', 'rf_optimized_advanced']\n",
      "   Best strategy: equal_weights\n",
      "   Training R2: 0.991305\n",
      "   Training RMSE: 158.6118\n",
      "   Sophisticated ensembles: ['stacking_neural_network', 'stacking_elastic_net', 'stacking_ridge', 'stacking_svr', 'voting_performance_squared', 'voting_performance_cubed', 'voting_softmax_performance', 'voting_rank_based_exponential', 'high_performer_ensemble', 'neural_adaptive', 'tree_adaptive']\n",
      "\n",
      "Production pipeline ready for predictions!\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing production pipeline...\")\n",
    "\n",
    "# Set up paths to production models\n",
    "models_directory = \"../production_models_final\"\n",
    "\n",
    "# Find the latest ensemble configuration file\n",
    "config_files = [f for f in os.listdir(models_directory) if f.startswith('ensemble_config_')]\n",
    "if not config_files:\n",
    "    raise ValueError(f\"No ensemble config found in {models_directory}\")\n",
    "\n",
    "latest_config = max(config_files)  # Get the latest config file\n",
    "config_path = os.path.join(models_directory, latest_config)\n",
    "\n",
    "print(f\"Using configuration: {latest_config}\")\n",
    "print(f\"Models directory: {models_directory}\")\n",
    "\n",
    "# Initialize the production pipeline\n",
    "pipeline = BigMartProductionPipeline()\n",
    "\n",
    "# Load the pipeline with trained models\n",
    "pipeline.load_pipeline(config_path, models_directory)\n",
    "\n",
    "# Get pipeline information\n",
    "pipeline_info = pipeline.get_pipeline_info()\n",
    "print(f\"\\nPipeline Information:\")\n",
    "print(f\"   Loaded models: {pipeline_info['loaded_models']}\")\n",
    "print(f\"   Best strategy: {pipeline_info['best_strategy']}\")\n",
    "print(f\"   Training R2: {pipeline_info['training_performance']['r2_score']:.6f}\")\n",
    "print(f\"   Training RMSE: {pipeline_info['training_performance']['rmse']:.4f}\")\n",
    "print(f\"   Sophisticated ensembles: {pipeline_info['sophisticated_ensembles']}\")\n",
    "\n",
    "print(f\"\\nProduction pipeline ready for predictions!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8446b320",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions on test data...\n",
      "Transforming data with BigMartPreprocessor...\n",
      "Handling missing values with smart imputation...\n",
      "  - Imputing Item_Weight using multi-level groupby strategy...\n",
      "  - Imputing Item_Weight using multi-level groupby strategy...\n",
      "    ✓ Item_Weight imputed (remaining NaNs: 0)\n",
      "  - Imputing Outlet_Size using outlet type and location patterns...\n",
      "    ✓ Outlet_Size imputed (remaining NaNs: 0)\n",
      "  - Checking for other missing values...\n",
      "    - Found 353 zero Item_Visibility values, replacing with Item_Type median...\n",
      "    ✓ Item_Visibility zeros handled (remaining zeros: 0)\n",
      "     Smart missing value imputation completed!\n",
      "  Creating engineered features...\n",
      "Adding statistical features...\n",
      "Encoding categorical variables...\n",
      "Final data cleanup...\n",
      "Transformation complete! Final shape: (5681, 48)\n",
      "Predictions generated successfully\n",
      "Processed data shape: (5681, 48)\n",
      "\n",
      "Prediction Methods Available:\n",
      "   Weighted Ensemble: 5681 predictions\n",
      "   Neural Adaptive: 5681 predictions\n",
      "   Individual models: ['et_optimized_advanced', 'gb_optimized_advanced', 'xgb_optimized_advanced', 'rf_optimized_advanced']\n",
      "\n",
      "Using Neural Adaptive for final predictions\n",
      "\n",
      "Prediction Statistics:\n",
      "   Mean prediction: 2129.90\n",
      "   Std prediction: 1402.31\n",
      "   Min prediction: -143.76\n",
      "   Max prediction: 9574.84\n",
      "\n",
      "Prediction Quality Check:\n",
      "   Negative predictions: 15\n",
      "   Very high predictions (>10,000): 0\n",
      "   Note: Some predictions may need review\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\main_content\\public_Hacathons\\Bigmart_sales\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2742: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(\"Generating predictions on test data...\")\n",
    "\n",
    "# Generate complete predictions using the production pipeline\n",
    "test_results = pipeline.predict_complete(test_data)\n",
    "\n",
    "print(f\"Predictions generated successfully\")\n",
    "print(f\"Processed data shape: {test_results['data_shape']}\")\n",
    "\n",
    "# Extract different prediction types\n",
    "weighted_ensemble_predictions = test_results['weighted_ensemble']['ensemble_prediction']\n",
    "neural_adaptive_predictions = test_results['neural_adaptive']\n",
    "individual_predictions = test_results['weighted_ensemble']['individual_predictions']\n",
    "\n",
    "print(f\"\\nPrediction Methods Available:\")\n",
    "print(f\"   Weighted Ensemble: {len(weighted_ensemble_predictions)} predictions\")\n",
    "print(f\"   Neural Adaptive: {len(neural_adaptive_predictions) if neural_adaptive_predictions is not None else 'Not available'} predictions\")\n",
    "print(f\"   Individual models: {list(individual_predictions.keys())}\")\n",
    "\n",
    "# Use the champion model (Neural Adaptive) for final predictions\n",
    "final_predictions = neural_adaptive_predictions if neural_adaptive_predictions is not None else weighted_ensemble_predictions\n",
    "prediction_method = \"Neural Adaptive\" if neural_adaptive_predictions is not None else \"Weighted Ensemble\"\n",
    "\n",
    "print(f\"\\nUsing {prediction_method} for final predictions\")\n",
    "\n",
    "# Display prediction statistics\n",
    "print(f\"\\nPrediction Statistics:\")\n",
    "print(f\"   Mean prediction: {np.mean(final_predictions):.2f}\")\n",
    "print(f\"   Std prediction: {np.std(final_predictions):.2f}\")\n",
    "print(f\"   Min prediction: {np.min(final_predictions):.2f}\")\n",
    "print(f\"   Max prediction: {np.max(final_predictions):.2f}\")\n",
    "\n",
    "# Check for any unusual predictions\n",
    "unusual_low = np.sum(final_predictions < 0)\n",
    "unusual_high = np.sum(final_predictions > 10000)\n",
    "\n",
    "print(f\"\\nPrediction Quality Check:\")\n",
    "print(f\"   Negative predictions: {unusual_low}\")\n",
    "print(f\"   Very high predictions (>10,000): {unusual_high}\")\n",
    "\n",
    "if unusual_low > 0 or unusual_high > 0:\n",
    "    print(\"   Note: Some predictions may need review\")\n",
    "else:\n",
    "    print(\"   All predictions within reasonable range\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b41bb22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing predictions to handle negative values...\n",
      "Found 15 negative predictions that need correction\n",
      "Negative prediction statistics:\n",
      "   Most negative: -143.76\n",
      "   Mean of negatives: -22.84\n",
      "\n",
      "Post-processing results:\n",
      "   Original negative predictions: 15\n",
      "   Remaining negative predictions: 0\n",
      "   Status: All negative predictions successfully corrected\n",
      "\n",
      "Updated prediction statistics:\n",
      "   Mean prediction: 2130.27\n",
      "   Min prediction: 13.54\n",
      "   Max prediction: 9574.84\n",
      "\n",
      "Correction details:\n",
      "   Number of corrections: 15\n",
      "   Corrected values range: [31.50, 237.21]\n",
      "Post-processing completed\n"
     ]
    }
   ],
   "source": [
    "print(\"Post-processing predictions to handle negative values...\")\n",
    "\n",
    "# Identify negative predictions\n",
    "negative_mask = final_predictions < 0\n",
    "num_negative = np.sum(negative_mask)\n",
    "\n",
    "print(f\"Found {num_negative} negative predictions that need correction\")\n",
    "\n",
    "if num_negative > 0:\n",
    "    print(f\"Negative prediction statistics:\")\n",
    "    negative_predictions = final_predictions[negative_mask]\n",
    "    print(f\"   Most negative: {np.min(negative_predictions):.2f}\")\n",
    "    print(f\"   Mean of negatives: {np.mean(negative_predictions):.2f}\")\n",
    "    \n",
    "    # Strategy 1: Replace with small positive values based on similar items\n",
    "    # Get the weighted ensemble predictions for comparison\n",
    "    weighted_negatives = weighted_ensemble_predictions[negative_mask]\n",
    "    \n",
    "    # Strategy 2: Use a combination of approaches\n",
    "    corrected_predictions = final_predictions.copy()\n",
    "    \n",
    "    for i, is_negative in enumerate(negative_mask):\n",
    "        if is_negative:\n",
    "            # Option 1: Use weighted ensemble if it's positive\n",
    "            if weighted_ensemble_predictions[i] > 0:\n",
    "                corrected_predictions[i] = weighted_ensemble_predictions[i]\n",
    "            # Option 2: Use individual model predictions (take the median of positive ones)\n",
    "            else:\n",
    "                individual_preds = [individual_predictions[model][i] for model in individual_predictions.keys()]\n",
    "                positive_preds = [p for p in individual_preds if p > 0]\n",
    "                \n",
    "                if positive_preds:\n",
    "                    corrected_predictions[i] = np.median(positive_preds)\n",
    "                else:\n",
    "                    # Option 3: Use a small positive value based on similar items\n",
    "                    # Set to 10th percentile of all positive predictions\n",
    "                    corrected_predictions[i] = np.percentile(final_predictions[final_predictions > 0], 10)\n",
    "    \n",
    "    # Verify correction\n",
    "    remaining_negative = np.sum(corrected_predictions < 0)\n",
    "    print(f\"\\nPost-processing results:\")\n",
    "    print(f\"   Original negative predictions: {num_negative}\")\n",
    "    print(f\"   Remaining negative predictions: {remaining_negative}\")\n",
    "    \n",
    "    if remaining_negative == 0:\n",
    "        print(f\"   Status: All negative predictions successfully corrected\")\n",
    "        \n",
    "        # Update final predictions\n",
    "        final_predictions = corrected_predictions\n",
    "        \n",
    "        # Update prediction statistics\n",
    "        print(f\"\\nUpdated prediction statistics:\")\n",
    "        print(f\"   Mean prediction: {np.mean(final_predictions):.2f}\")\n",
    "        print(f\"   Min prediction: {np.min(final_predictions):.2f}\")\n",
    "        print(f\"   Max prediction: {np.max(final_predictions):.2f}\")\n",
    "        \n",
    "        # Show correction details\n",
    "        correction_applied = negative_mask\n",
    "        if np.sum(correction_applied) > 0:\n",
    "            print(f\"\\nCorrection details:\")\n",
    "            print(f\"   Number of corrections: {np.sum(correction_applied)}\")\n",
    "            corrected_values = final_predictions[correction_applied]\n",
    "            print(f\"   Corrected values range: [{np.min(corrected_values):.2f}, {np.max(corrected_values):.2f}]\")\n",
    "    else:\n",
    "        print(f\"   Warning: {remaining_negative} predictions still negative - additional review needed\")\n",
    "\n",
    "else:\n",
    "    print(\"No negative predictions found - no correction needed\")\n",
    "\n",
    "print(\"Post-processing completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de89f304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating submission file...\n",
      "Submission file saved: bigmart_predictions_neural_adaptive_20250907_123308.csv\n",
      "Prediction method used: Neural Adaptive\n",
      "Number of predictions: 5681\n",
      "\n",
      "First 10 predictions:\n",
      "  Item_Identifier Outlet_Identifier  Item_Outlet_Sales\n",
      "0           FDW58            OUT049        1474.690858\n",
      "1           FDW14            OUT017        1410.542196\n",
      "2           NCN55            OUT010        1117.430953\n",
      "3           FDQ58            OUT017        2500.559481\n",
      "4           FDY38            OUT027        6206.035277\n",
      "5           FDH56            OUT046        2393.738943\n",
      "6           FDL48            OUT018         208.861138\n",
      "7           FDC48            OUT027        2116.205241\n",
      "8           FDN33            OUT045         856.188666\n",
      "9           FDA36            OUT017        2274.243682\n",
      "Detailed predictions saved: detailed_predictions_20250907_123308.csv\n",
      "Confidence scores saved: prediction_confidence_20250907_123308.csv\n",
      "Mean confidence: 0.0066\n",
      "\n",
      "Prediction generation complete!\n",
      "Files created:\n",
      "   1. bigmart_predictions_neural_adaptive_20250907_123308.csv (main submission)\n",
      "   2. detailed_predictions_20250907_123308.csv (detailed analysis)\n",
      "   3. prediction_confidence_20250907_123308.csv (confidence scores)\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating submission file...\")\n",
    "\n",
    "# Check if test data has Item_Identifier and Outlet_Identifier for submission\n",
    "required_columns = ['Item_Identifier', 'Outlet_Identifier']\n",
    "missing_cols = [col for col in required_columns if col not in test_data.columns]\n",
    "\n",
    "if missing_cols:\n",
    "    print(f\"Warning: Missing required columns for submission: {missing_cols}\")\n",
    "    # Create a simple submission with index\n",
    "    submission_df = pd.DataFrame({\n",
    "        'ID': range(len(final_predictions)),\n",
    "        'Item_Outlet_Sales': final_predictions\n",
    "    })\n",
    "else:\n",
    "    # Create proper submission format\n",
    "    submission_df = pd.DataFrame({\n",
    "        'Item_Identifier': test_data['Item_Identifier'],\n",
    "        'Outlet_Identifier': test_data['Outlet_Identifier'],\n",
    "        'Item_Outlet_Sales': final_predictions\n",
    "    })\n",
    "\n",
    "# Generate timestamp for file naming\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save submission file\n",
    "submission_file = f\"bigmart_predictions_{prediction_method.lower().replace(' ', '_')}_{timestamp}.csv\"\n",
    "submission_df.to_csv(submission_file, index=False)\n",
    "\n",
    "print(f\"Submission file saved: {submission_file}\")\n",
    "print(f\"Prediction method used: {prediction_method}\")\n",
    "print(f\"Number of predictions: {len(final_predictions)}\")\n",
    "\n",
    "# Display first few predictions\n",
    "print(f\"\\nFirst 10 predictions:\")\n",
    "print(submission_df.head(10))\n",
    "\n",
    "# Save detailed results for analysis\n",
    "detailed_results = pd.DataFrame({\n",
    "    'Weighted_Ensemble': weighted_ensemble_predictions,\n",
    "    'Neural_Adaptive': neural_adaptive_predictions if neural_adaptive_predictions is not None else np.nan,\n",
    "    'Final_Prediction': final_predictions\n",
    "})\n",
    "\n",
    "# Add individual model predictions\n",
    "for model_name, predictions in individual_predictions.items():\n",
    "    detailed_results[f'Individual_{model_name}'] = predictions\n",
    "\n",
    "# Save detailed results\n",
    "detailed_file = f\"detailed_predictions_{timestamp}.csv\"\n",
    "detailed_results.to_csv(detailed_file, index=False)\n",
    "\n",
    "print(f\"Detailed predictions saved: {detailed_file}\")\n",
    "\n",
    "# Save confidence scores if available\n",
    "if 'confidence' in test_results['weighted_ensemble']:\n",
    "    confidence_scores = test_results['weighted_ensemble']['confidence']\n",
    "    confidence_df = pd.DataFrame({\n",
    "        'Prediction_Confidence': confidence_scores,\n",
    "        'Final_Prediction': final_predictions\n",
    "    })\n",
    "    \n",
    "    confidence_file = f\"prediction_confidence_{timestamp}.csv\"\n",
    "    confidence_df.to_csv(confidence_file, index=False)\n",
    "    \n",
    "    print(f\"Confidence scores saved: {confidence_file}\")\n",
    "    print(f\"Mean confidence: {np.mean(confidence_scores):.4f}\")\n",
    "\n",
    "print(f\"\\nPrediction generation complete!\")\n",
    "print(f\"Files created:\")\n",
    "print(f\"   1. {submission_file} (main submission)\")\n",
    "print(f\"   2. {detailed_file} (detailed analysis)\")\n",
    "if 'confidence' in test_results['weighted_ensemble']:\n",
    "    print(f\"   3. {confidence_file} (confidence scores)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6532dd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final validation and summary...\n",
      "VALIDATION SUMMARY:\n",
      "========================================\n",
      "Data consistency checks:\n",
      "   Test data samples: 5681\n",
      "   Generated predictions: 5681\n",
      "   Data consistency: PASS\n",
      "\n",
      "Prediction statistics:\n",
      "   Mean: 2130.27\n",
      "   Median: 1946.19\n",
      "   Standard deviation: 1401.78\n",
      "   Range: [13.54, 9574.84]\n",
      "\n",
      "Model performance reference (from training):\n",
      "   Training R2: 0.991305\n",
      "   Training RMSE: 158.6118\n",
      "   Method used: equal_weights\n",
      "\n",
      "Data quality checks:\n",
      "   NaN predictions: 0\n",
      "   Infinite predictions: 0\n",
      "   Data quality: PASS\n",
      "\n",
      "STATUS: PREDICTIONS READY FOR SUBMISSION\n",
      "Main submission file: bigmart_predictions_neural_adaptive_20250907_123308.csv\n",
      "\n",
      "Production pipeline execution completed successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"Final validation and summary...\")\n",
    "\n",
    "# Validation checks\n",
    "print(f\"VALIDATION SUMMARY:\")\n",
    "print(f\"=\" * 40)\n",
    "\n",
    "# Check data consistency\n",
    "print(f\"Data consistency checks:\")\n",
    "print(f\"   Test data samples: {len(test_data)}\")\n",
    "print(f\"   Generated predictions: {len(final_predictions)}\")\n",
    "print(f\"   Data consistency: {'PASS' if len(test_data) == len(final_predictions) else 'FAIL'}\")\n",
    "\n",
    "# Check prediction reasonableness\n",
    "pred_mean = np.mean(final_predictions)\n",
    "pred_median = np.median(final_predictions)\n",
    "pred_std = np.std(final_predictions)\n",
    "\n",
    "print(f\"\\nPrediction statistics:\")\n",
    "print(f\"   Mean: {pred_mean:.2f}\")\n",
    "print(f\"   Median: {pred_median:.2f}\")\n",
    "print(f\"   Standard deviation: {pred_std:.2f}\")\n",
    "print(f\"   Range: [{np.min(final_predictions):.2f}, {np.max(final_predictions):.2f}]\")\n",
    "\n",
    "# Model performance reference\n",
    "training_performance = pipeline_info['training_performance']\n",
    "print(f\"\\nModel performance reference (from training):\")\n",
    "print(f\"   Training R2: {training_performance['r2_score']:.6f}\")\n",
    "print(f\"   Training RMSE: {training_performance['rmse']:.4f}\")\n",
    "print(f\"   Method used: {pipeline_info['best_strategy']}\")\n",
    "\n",
    "# Check for any data quality issues\n",
    "print(f\"\\nData quality checks:\")\n",
    "nan_predictions = np.sum(np.isnan(final_predictions))\n",
    "inf_predictions = np.sum(np.isinf(final_predictions))\n",
    "\n",
    "print(f\"   NaN predictions: {nan_predictions}\")\n",
    "print(f\"   Infinite predictions: {inf_predictions}\")\n",
    "print(f\"   Data quality: {'PASS' if nan_predictions == 0 and inf_predictions == 0 else 'FAIL'}\")\n",
    "\n",
    "# Final status\n",
    "if (len(test_data) == len(final_predictions) and \n",
    "    nan_predictions == 0 and inf_predictions == 0):\n",
    "    print(f\"\\nSTATUS: PREDICTIONS READY FOR SUBMISSION\")\n",
    "    print(f\"Main submission file: {submission_file}\")\n",
    "else:\n",
    "    print(f\"\\nSTATUS: REVIEW REQUIRED\")\n",
    "    print(f\"Please check data quality issues above\")\n",
    "\n",
    "print(f\"\\nProduction pipeline execution completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
