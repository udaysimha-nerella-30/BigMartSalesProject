{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5778cd3",
   "metadata": {},
   "source": [
    "# üî¨ BigMart Sales - Proper Train/Validation Split Approach\n",
    "\n",
    "This notebook implements a robust validation methodology by:\n",
    "\n",
    "1. **Loading original training data**\n",
    "2. **Creating proper train/validation splits** \n",
    "3. **Saving splits separately** (train_splitted.csv, validation_splitted.csv)\n",
    "4. **Training only on train_splitted.csv**\n",
    "5. **Validating on validation_splitted.csv** (unseen data)\n",
    "\n",
    "This approach ensures true validation performance by eliminating any potential data leakage between training and validation phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23d85960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ BigMart Sales - Proper Train/Validation Split Approach\n",
      "============================================================\n",
      "‚úÖ Libraries imported successfully\n",
      "üé≤ Random state set to: 42\n"
     ]
    }
   ],
   "source": [
    "# 1. Import Libraries and Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score, GroupKFold, RandomizedSearchCV, GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import KNNImputer\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random state for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"üî¨ BigMart Sales - Proper Train/Validation Split Approach\")\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"üé≤ Random state set to: {RANDOM_STATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbe9da68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Loading Original Data, Creating Split, and Saving...\n",
      "------------------------------------------------------------\n",
      "‚úÖ Original training data loaded: (8523, 12)\n",
      "‚úÖ Data split created:\n",
      "   ‚Ä¢ Training split: (6818, 12) (80.0%)\n",
      "   ‚Ä¢ Validation split: (1705, 12) (20.0%)\n",
      "   ‚Ä¢ Overlap: 0 (should be 0) - ‚úÖ GOOD\n",
      "\n",
      "üíæ Files Saved:\n",
      "   ‚Ä¢ train_data_splitted.csv\n",
      "   ‚Ä¢ validation_data_splitted.csv\n",
      "\n",
      "üßπ Memory cleared - ready for clean training process\n"
     ]
    }
   ],
   "source": [
    "# 2. Load Original Data, Create ONE Split, and Save to CSV Files\n",
    "print(\"üìä Loading Original Data, Creating Split, and Saving...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Load original training data\n",
    "train_data_raw = pd.read_csv('code/train_data.csv')\n",
    "print(f\"‚úÖ Original training data loaded: {train_data_raw.shape}\")\n",
    "\n",
    "# Create ONE train/validation split using GroupKFold\n",
    "cv_strategy = GroupKFold(n_splits=5)\n",
    "groups = train_data_raw['Item_Identifier']\n",
    "\n",
    "# Get the first split (80/20 split approximately)\n",
    "train_idx, val_idx = next(cv_strategy.split(train_data_raw, train_data_raw['Item_Outlet_Sales'], groups))\n",
    "\n",
    "# Create train and validation datasets\n",
    "train_data_split = train_data_raw.iloc[train_idx].copy()\n",
    "validation_data_split = train_data_raw.iloc[val_idx].copy()\n",
    "\n",
    "print(f\"‚úÖ Data split created:\")\n",
    "print(f\"   ‚Ä¢ Training split: {train_data_split.shape} ({len(train_idx)/len(train_data_raw)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Validation split: {validation_data_split.shape} ({len(val_idx)/len(train_data_raw)*100:.1f}%)\")\n",
    "\n",
    "# Verify no item overlap\n",
    "train_items = set(train_data_split['Item_Identifier'])\n",
    "val_items = set(validation_data_split['Item_Identifier'])\n",
    "overlap = train_items.intersection(val_items)\n",
    "print(f\"   ‚Ä¢ Overlap: {len(overlap)} (should be 0) - {'‚úÖ GOOD' if len(overlap) == 0 else '‚ùå BAD'}\")\n",
    "\n",
    "# Save splits to CSV files\n",
    "output_dir = Path(\"data_splits\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "train_split_path = output_dir / \"train_data_splitted.csv\"\n",
    "validation_split_path = output_dir / \"validation_data_splitted.csv\"\n",
    "\n",
    "train_data_split.to_csv(train_split_path, index=False)\n",
    "validation_data_split.to_csv(validation_split_path, index=False)\n",
    "\n",
    "print(f\"\\nüíæ Files Saved:\")\n",
    "print(f\"   ‚Ä¢ {train_split_path.name}\")\n",
    "print(f\"   ‚Ä¢ {validation_split_path.name}\")\n",
    "\n",
    "# Clear all data from memory\n",
    "del train_data_raw, train_data_split, validation_data_split, train_idx, val_idx, groups, train_items, val_items\n",
    "print(f\"\\nüßπ Memory cleared - ready for clean training process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c6d9fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ BigMartPreprocessor class defined\n"
     ]
    }
   ],
   "source": [
    "# 3. Define BigMartPreprocessor Class\n",
    "class BigMartPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Complete preprocessing pipeline for BigMart sales data\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.item_stats = None\n",
    "        self.outlet_stats = None\n",
    "        self.item_target_mean = None\n",
    "        self.overall_mean = None\n",
    "        self.outlet_size_mode = {}\n",
    "        self.is_fitted = False\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        if y is not None:\n",
    "            self.overall_mean = y.mean()\n",
    "            X_temp = X.copy()\n",
    "            X_temp['target'] = y\n",
    "            \n",
    "            # Item statistics\n",
    "            self.item_stats = X_temp.groupby('Item_Identifier')['target'].agg([\n",
    "                'mean', 'median', 'std', 'count'\n",
    "            ]).add_prefix('Item_')\n",
    "            \n",
    "            # Outlet statistics  \n",
    "            self.outlet_stats = X_temp.groupby('Outlet_Identifier')['target'].agg([\n",
    "                'mean', 'median', 'std', 'count'\n",
    "            ]).add_prefix('Outlet_')\n",
    "            \n",
    "            # Target encoding\n",
    "            self.item_target_mean = X_temp.groupby('Item_Identifier')['target'].mean().to_dict()\n",
    "        \n",
    "        # Outlet size mode by outlet type\n",
    "        if 'Outlet_Size' in X.columns and X['Outlet_Size'].isnull().any():\n",
    "            self.outlet_size_mode = X.groupby('Outlet_Type')['Outlet_Size'].apply(\n",
    "                lambda x: x.mode().iloc[0] if not x.mode().empty else 'Medium'\n",
    "            ).to_dict()\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_processed = X.copy()\n",
    "        \n",
    "        # 1. Handle missing values\n",
    "        if 'Item_Weight' in X_processed.columns and X_processed['Item_Weight'].isnull().any():\n",
    "            weight_median_by_type = X_processed.groupby('Item_Type')['Item_Weight'].median()\n",
    "            for item_type in X_processed['Item_Type'].unique():\n",
    "                mask = (X_processed['Item_Type'] == item_type) & X_processed['Item_Weight'].isnull()\n",
    "                if mask.any():\n",
    "                    median_val = weight_median_by_type.get(item_type, X_processed['Item_Weight'].median())\n",
    "                    X_processed.loc[mask, 'Item_Weight'] = median_val\n",
    "        \n",
    "        if 'Outlet_Size' in X_processed.columns and X_processed['Outlet_Size'].isnull().any():\n",
    "            for outlet_type, mode_size in self.outlet_size_mode.items():\n",
    "                mask = (X_processed['Outlet_Type'] == outlet_type) & X_processed['Outlet_Size'].isnull()\n",
    "                if mask.any():\n",
    "                    X_processed.loc[mask, 'Outlet_Size'] = mode_size\n",
    "        \n",
    "        # 2. Enhanced Item_Identifier features\n",
    "        X_processed['Item_Category'] = X_processed['Item_Identifier'].str[:2]\n",
    "        item_numeric = X_processed['Item_Identifier'].str[2:]\n",
    "        X_processed['Item_Number'] = pd.to_numeric(item_numeric, errors='coerce').fillna(0).astype(int)\n",
    "        \n",
    "        category_mapping = {'FD': 'Food', 'NC': 'Non-Consumable', 'DR': 'Drinks'}\n",
    "        X_processed['Item_Category_Group'] = X_processed['Item_Category'].map(category_mapping)\n",
    "        \n",
    "        # Target encoding\n",
    "        if self.item_target_mean is not None:\n",
    "            X_processed['Item_Target_Encoded'] = X_processed['Item_Identifier'].map(self.item_target_mean)\n",
    "            X_processed['Item_Target_Encoded'].fillna(self.overall_mean, inplace=True)\n",
    "        \n",
    "        # 3. Add statistics\n",
    "        if self.item_stats is not None:\n",
    "            X_processed = X_processed.merge(self.item_stats, left_on='Item_Identifier', right_index=True, how='left')\n",
    "        if self.outlet_stats is not None:\n",
    "            X_processed = X_processed.merge(self.outlet_stats, left_on='Outlet_Identifier', right_index=True, how='left')\n",
    "        \n",
    "        # 4. Feature engineering\n",
    "        X_processed['Item_MRP_Bin'] = pd.cut(X_processed['Item_MRP'], bins=4, labels=['Low', 'Medium', 'High', 'Premium'])\n",
    "        X_processed['Outlet_Age'] = 2013 - X_processed['Outlet_Establishment_Year']\n",
    "        X_processed['Outlet_Age_Group'] = pd.cut(X_processed['Outlet_Age'], bins=[0, 10, 20, 30], labels=['New', 'Medium', 'Old'])\n",
    "        X_processed['Item_Visibility_Binned'] = pd.cut(X_processed['Item_Visibility'], bins=5, labels=['Very_Low', 'Low', 'Medium', 'High', 'Very_High'])\n",
    "        \n",
    "        food_categories = ['Dairy', 'Soft Drinks', 'Meat', 'Fruits and Vegetables', \n",
    "                          'Household', 'Baking Goods', 'Snack Foods', 'Frozen Foods',\n",
    "                          'Breakfast', 'Health and Hygiene', 'Hard Drinks', 'Canned',\n",
    "                          'Breads', 'Starchy Foods', 'Others', 'Seafood']\n",
    "        X_processed['Item_Type_Category'] = X_processed['Item_Type'].apply(\n",
    "            lambda x: 'Food' if x in food_categories else 'Non-Food'\n",
    "        )\n",
    "        \n",
    "        # 5. Encode categorical variables\n",
    "        categorical_cols = X_processed.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        id_cols = ['Item_Identifier', 'Outlet_Identifier']\n",
    "        categorical_cols = [col for col in categorical_cols if col not in id_cols]\n",
    "        \n",
    "        if categorical_cols:\n",
    "            X_encoded = pd.get_dummies(X_processed, columns=categorical_cols, drop_first=True)\n",
    "        else:\n",
    "            X_encoded = X_processed\n",
    "        \n",
    "        # Keep Item_Identifier for GroupKFold, remove Outlet_Identifier\n",
    "        feature_cols = [col for col in X_encoded.columns if col != 'Outlet_Identifier']\n",
    "        X_final = X_encoded[feature_cols]\n",
    "        \n",
    "        return X_final\n",
    "\n",
    "print(\"‚úÖ BigMartPreprocessor class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cac21e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Loading Training Split and Starting Model Development...\n",
      "============================================================\n",
      "‚úÖ Training split loaded: (6818, 12)\n",
      "   ‚Ä¢ Unique items: 1247\n",
      "   ‚Ä¢ Target mean: $2166.09\n",
      "üìä Training Data:\n",
      "   ‚Ä¢ Features: (6818, 11)\n",
      "   ‚Ä¢ Target: (6818,)\n",
      "‚úÖ Preprocessing pipeline fitted\n",
      "‚úÖ Training data transformed: (6818, 11) ‚Üí (6818, 55)\n",
      "üìä Model data ready: (6818, 54)\n",
      "\n",
      "üîÄ Cross-Validation Setup (Training Split Only):\n",
      "   ‚Ä¢ Strategy: GroupKFold (5 splits)\n",
      "   ‚Ä¢ Groups: 1247 unique items\n",
      "   ‚Ä¢ Total records: 6818\n",
      "\n",
      "ü§ñ Performing Cross-Validation...\n",
      "\n",
      "üìä Cross-Validation Results:\n",
      "   ‚Ä¢ R¬≤ Mean: 0.6962 ¬± 0.0133\n",
      "   ‚Ä¢ RMSE Mean: $935.43 ¬± $27.64\n",
      "\n",
      "üéØ Training Final Model on Complete Training Split...\n",
      "‚úÖ Final model trained\n",
      "üíæ Model saved: best_model_20250906_160015.pkl\n",
      "üíæ Pipeline saved: preprocessor_20250906_160015.pkl\n",
      "\n",
      "üéØ Training Complete - Ready for Validation Test!\n",
      "   ‚Ä¢ Cross-validation R¬≤: 0.6962\n",
      "   ‚Ä¢ Cross-validation RMSE: $935.43\n",
      "   ‚Ä¢ Next: Load validation split and test final performance\n"
     ]
    }
   ],
   "source": [
    "# 4. Load ONLY Training Split and Perform Complete Model Development\n",
    "print(\"üöÄ Loading Training Split and Starting Model Development...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load ONLY the training split\n",
    "train_data = pd.read_csv(\"data_splits/train_data_splitted.csv\")\n",
    "print(f\"‚úÖ Training split loaded: {train_data.shape}\")\n",
    "print(f\"   ‚Ä¢ Unique items: {train_data['Item_Identifier'].nunique()}\")\n",
    "print(f\"   ‚Ä¢ Target mean: ${train_data['Item_Outlet_Sales'].mean():.2f}\")\n",
    "\n",
    "# Prepare features and target\n",
    "X_train_raw = train_data.drop('Item_Outlet_Sales', axis=1)\n",
    "y_train = train_data['Item_Outlet_Sales']\n",
    "\n",
    "print(f\"üìä Training Data:\")\n",
    "print(f\"   ‚Ä¢ Features: {X_train_raw.shape}\")\n",
    "print(f\"   ‚Ä¢ Target: {y_train.shape}\")\n",
    "\n",
    "# Fit preprocessing pipeline on training split only\n",
    "preprocessor = BigMartPreprocessor()\n",
    "preprocessor.fit(X_train_raw, y_train)\n",
    "print(\"‚úÖ Preprocessing pipeline fitted\")\n",
    "\n",
    "# Transform training data\n",
    "X_train_processed = preprocessor.transform(X_train_raw)\n",
    "print(f\"‚úÖ Training data transformed: {X_train_raw.shape} ‚Üí {X_train_processed.shape}\")\n",
    "\n",
    "# Prepare data for modeling (remove Item_Identifier)\n",
    "X_model = X_train_processed.drop('Item_Identifier', axis=1) if 'Item_Identifier' in X_train_processed.columns else X_train_processed\n",
    "print(f\"üìä Model data ready: {X_model.shape}\")\n",
    "\n",
    "# Setup GroupKFold cross-validation on training split\n",
    "cv_strategy = GroupKFold(n_splits=5)\n",
    "cv_groups = X_train_processed['Item_Identifier'] if 'Item_Identifier' in X_train_processed.columns else X_train_raw['Item_Identifier']\n",
    "\n",
    "print(f\"\\nüîÄ Cross-Validation Setup (Training Split Only):\")\n",
    "print(f\"   ‚Ä¢ Strategy: GroupKFold (5 splits)\")\n",
    "print(f\"   ‚Ä¢ Groups: {cv_groups.nunique()} unique items\")\n",
    "print(f\"   ‚Ä¢ Total records: {len(X_model)}\")\n",
    "\n",
    "# Train Random Forest model with cross-validation\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"\\nü§ñ Performing Cross-Validation...\")\n",
    "cv_scores_r2 = cross_val_score(rf_model, X_model, y_train, cv=cv_strategy, \n",
    "                               groups=cv_groups, scoring='r2', n_jobs=-1)\n",
    "cv_scores_neg_rmse = cross_val_score(rf_model, X_model, y_train, cv=cv_strategy, \n",
    "                                     groups=cv_groups, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "cv_scores_rmse = -cv_scores_neg_rmse\n",
    "\n",
    "print(f\"\\nüìä Cross-Validation Results:\")\n",
    "print(f\"   ‚Ä¢ R¬≤ Mean: {cv_scores_r2.mean():.4f} ¬± {cv_scores_r2.std():.4f}\")\n",
    "print(f\"   ‚Ä¢ RMSE Mean: ${cv_scores_rmse.mean():.2f} ¬± ${cv_scores_rmse.std():.2f}\")\n",
    "\n",
    "# Train final model on full training split\n",
    "print(f\"\\nüéØ Training Final Model on Complete Training Split...\")\n",
    "rf_model.fit(X_model, y_train)\n",
    "print(\"‚úÖ Final model trained\")\n",
    "\n",
    "# Save the model and pipeline\n",
    "models_dir = Path(\"trained_models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_path = models_dir / f\"best_model_{timestamp}.pkl\"\n",
    "pipeline_path = models_dir / f\"preprocessor_{timestamp}.pkl\"\n",
    "\n",
    "joblib.dump(rf_model, model_path)\n",
    "joblib.dump(preprocessor, pipeline_path)\n",
    "\n",
    "print(f\"üíæ Model saved: {model_path.name}\")\n",
    "print(f\"üíæ Pipeline saved: {pipeline_path.name}\")\n",
    "\n",
    "print(f\"\\nüéØ Training Complete - Ready for Validation Test!\")\n",
    "print(f\"   ‚Ä¢ Cross-validation R¬≤: {cv_scores_r2.mean():.4f}\")\n",
    "print(f\"   ‚Ä¢ Cross-validation RMSE: ${cv_scores_rmse.mean():.2f}\")\n",
    "print(f\"   ‚Ä¢ Next: Load validation split and test final performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b62a0b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Loading Validation Split and Testing Final Model...\n",
      "============================================================\n",
      "‚úÖ Validation split loaded: (1705, 12)\n",
      "   ‚Ä¢ Unique items: 312\n",
      "üìä Validation Data:\n",
      "   ‚Ä¢ Features: (1705, 11)\n",
      "   ‚Ä¢ Target: (1705,)\n",
      "   ‚Ä¢ Target mean: $2242.07\n",
      "\n",
      "üîß Applying Preprocessing Pipeline...\n",
      "‚úÖ Validation data transformed: (1705, 11) ‚Üí (1705, 55)\n",
      "üìä Validation model data: (1705, 54)\n",
      "\n",
      "üîÆ Making Predictions...\n",
      "\n",
      "üìà FINAL VALIDATION RESULTS:\n",
      "==================================================\n",
      "üìä R¬≤ Score: 0.2088\n",
      "üí∞ RMSE: $1535.87\n",
      "üìä MAE: $1064.87\n",
      "\n",
      "üìä Cross-Validation vs Validation Comparison:\n",
      "   ‚Ä¢ CV R¬≤: 0.6962 ‚Üí Validation R¬≤: 0.2088\n",
      "   ‚Ä¢ CV RMSE: $935.43 ‚Üí Validation RMSE: $1535.87\n",
      "\n",
      "üéØ Performance Assessment:\n",
      "‚ö†Ô∏è CONCERNS: Validation performance below expectations\n",
      "   ‚Ä¢ May need model improvement\n",
      "\n",
      "üéâ VALIDATION COMPLETE!\n",
      "   ‚Ä¢ True validation on completely unseen data\n",
      "   ‚Ä¢ No data leakage between training and validation\n",
      "   ‚Ä¢ Robust performance assessment\n"
     ]
    }
   ],
   "source": [
    "# 5. Load Validation Split and Test Final Model Performance\n",
    "print(\"üß™ Loading Validation Split and Testing Final Model...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load validation split (completely unseen data)\n",
    "validation_data = pd.read_csv(\"data_splits/validation_data_splitted.csv\")\n",
    "print(f\"‚úÖ Validation split loaded: {validation_data.shape}\")\n",
    "print(f\"   ‚Ä¢ Unique items: {validation_data['Item_Identifier'].nunique()}\")\n",
    "\n",
    "# Prepare validation features and target\n",
    "X_val_raw = validation_data.drop('Item_Outlet_Sales', axis=1)\n",
    "y_val = validation_data['Item_Outlet_Sales']\n",
    "\n",
    "print(f\"üìä Validation Data:\")\n",
    "print(f\"   ‚Ä¢ Features: {X_val_raw.shape}\")\n",
    "print(f\"   ‚Ä¢ Target: {y_val.shape}\")\n",
    "print(f\"   ‚Ä¢ Target mean: ${y_val.mean():.2f}\")\n",
    "\n",
    "# Apply preprocessing pipeline (fitted on training data only)\n",
    "print(f\"\\nüîß Applying Preprocessing Pipeline...\")\n",
    "X_val_processed = preprocessor.transform(X_val_raw)\n",
    "print(f\"‚úÖ Validation data transformed: {X_val_raw.shape} ‚Üí {X_val_processed.shape}\")\n",
    "\n",
    "# Prepare for model prediction (remove Item_Identifier)\n",
    "X_val_model = X_val_processed.drop('Item_Identifier', axis=1) if 'Item_Identifier' in X_val_processed.columns else X_val_processed\n",
    "print(f\"üìä Validation model data: {X_val_model.shape}\")\n",
    "\n",
    "# Make predictions\n",
    "print(f\"\\nüîÆ Making Predictions...\")\n",
    "y_pred = rf_model.predict(X_val_model)\n",
    "\n",
    "# Calculate metrics\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "\n",
    "print(f\"\\nüìà FINAL VALIDATION RESULTS:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìä R¬≤ Score: {r2:.4f}\")\n",
    "print(f\"üí∞ RMSE: ${rmse:.2f}\")\n",
    "print(f\"üìä MAE: ${mae:.2f}\")\n",
    "\n",
    "print(f\"\\nüìä Cross-Validation vs Validation Comparison:\")\n",
    "print(f\"   ‚Ä¢ CV R¬≤: {cv_scores_r2.mean():.4f} ‚Üí Validation R¬≤: {r2:.4f}\")\n",
    "print(f\"   ‚Ä¢ CV RMSE: ${cv_scores_rmse.mean():.2f} ‚Üí Validation RMSE: ${rmse:.2f}\")\n",
    "\n",
    "# Performance assessment\n",
    "print(f\"\\nüéØ Performance Assessment:\")\n",
    "if abs(r2 - cv_scores_r2.mean()) <= 0.05 and abs(rmse - cv_scores_rmse.mean()) <= 100:\n",
    "    print(\"‚úÖ EXCELLENT! Validation performance matches cross-validation\")\n",
    "    print(\"   ‚Ä¢ No overfitting detected\")\n",
    "    print(\"   ‚Ä¢ Model generalizes well to unseen data\")\n",
    "elif r2 >= 0.65 and rmse <= 1000:\n",
    "    print(\"‚úÖ GOOD! Validation performance is acceptable\")\n",
    "    print(\"   ‚Ä¢ Meets performance thresholds\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è CONCERNS: Validation performance below expectations\")\n",
    "    print(\"   ‚Ä¢ May need model improvement\")\n",
    "\n",
    "print(f\"\\nüéâ VALIDATION COMPLETE!\")\n",
    "print(f\"   ‚Ä¢ True validation on completely unseen data\")\n",
    "print(f\"   ‚Ä¢ No data leakage between training and validation\")\n",
    "print(f\"   ‚Ä¢ Robust performance assessment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba935fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving Baseline Models and Results...\n",
      "============================================================\n",
      "‚úÖ Baseline model saved: baseline_model_20250906_160544.pkl\n",
      "‚úÖ Baseline pipeline saved: baseline_preprocessor_20250906_160544.pkl\n",
      "‚úÖ Baseline results saved: baseline_results_20250906_160544.json\n",
      "‚úÖ Baseline summary saved: baseline_summary_20250906_160544.md\n",
      "\n",
      "üéØ BASELINE ESTABLISHED!\n",
      "==================================================\n",
      "üìä Cross-Validation Performance: R¬≤ = 0.6962, RMSE = $935.43\n",
      "üìä Validation Performance: R¬≤ = 0.2088, RMSE = $1535.87\n",
      "üö® Performance Gap: R¬≤ drops by 0.4874\n",
      "\n",
      "üìÅ Standard Data Files:\n",
      "   ‚Ä¢ Training: data_splits/train_data_splitted.csv\n",
      "   ‚Ä¢ Validation: data_splits/validation_data_splitted.csv\n",
      "\n",
      "üöÄ Ready for model improvement iterations!\n",
      "   ‚Ä¢ Use baseline as comparison point\n",
      "   ‚Ä¢ Focus on reducing overfitting\n",
      "   ‚Ä¢ Improve generalization to unseen items\n"
     ]
    }
   ],
   "source": [
    "# 6. Save Baseline Models and Results for Future Improvement\n",
    "print(\"üíæ Saving Baseline Models and Results...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create baseline directory\n",
    "baseline_dir = Path(\"baseline_models\")\n",
    "baseline_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Generate timestamp for baseline\n",
    "baseline_timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save models with baseline naming\n",
    "baseline_model_path = baseline_dir / f\"baseline_model_{baseline_timestamp}.pkl\"\n",
    "baseline_pipeline_path = baseline_dir / f\"baseline_preprocessor_{baseline_timestamp}.pkl\"\n",
    "\n",
    "joblib.dump(rf_model, baseline_model_path)\n",
    "joblib.dump(preprocessor, baseline_pipeline_path)\n",
    "\n",
    "print(f\"‚úÖ Baseline model saved: {baseline_model_path.name}\")\n",
    "print(f\"‚úÖ Baseline pipeline saved: {baseline_pipeline_path.name}\")\n",
    "\n",
    "# Save baseline performance results\n",
    "baseline_results = {\n",
    "    \"timestamp\": baseline_timestamp,\n",
    "    \"model_type\": \"RandomForestRegressor\",\n",
    "    \"random_state\": RANDOM_STATE,\n",
    "    \"data_split\": {\n",
    "        \"method\": \"GroupKFold\",\n",
    "        \"train_records\": len(train_data),\n",
    "        \"validation_records\": len(validation_data),\n",
    "        \"train_items\": train_data['Item_Identifier'].nunique(),\n",
    "        \"validation_items\": validation_data['Item_Identifier'].nunique()\n",
    "    },\n",
    "    \"cross_validation\": {\n",
    "        \"cv_strategy\": \"GroupKFold_5_splits\",\n",
    "        \"r2_mean\": float(cv_scores_r2.mean()),\n",
    "        \"r2_std\": float(cv_scores_r2.std()),\n",
    "        \"rmse_mean\": float(cv_scores_rmse.mean()),\n",
    "        \"rmse_std\": float(cv_scores_rmse.std()),\n",
    "        \"individual_folds\": {\n",
    "            \"r2_scores\": cv_scores_r2.tolist(),\n",
    "            \"rmse_scores\": cv_scores_rmse.tolist()\n",
    "        }\n",
    "    },\n",
    "    \"validation_performance\": {\n",
    "        \"r2_score\": float(r2),\n",
    "        \"rmse\": float(rmse),\n",
    "        \"mae\": float(mae),\n",
    "        \"target_mean_train\": float(train_data['Item_Outlet_Sales'].mean()),\n",
    "        \"target_mean_validation\": float(validation_data['Item_Outlet_Sales'].mean())\n",
    "    },\n",
    "    \"performance_gap\": {\n",
    "        \"r2_gap\": float(cv_scores_r2.mean() - r2),\n",
    "        \"rmse_gap\": float(rmse - cv_scores_rmse.mean()),\n",
    "        \"overfitting_detected\": bool(abs(cv_scores_r2.mean() - r2) > 0.05)\n",
    "    },\n",
    "    \"model_parameters\": {\n",
    "        \"n_estimators\": rf_model.n_estimators,\n",
    "        \"max_depth\": rf_model.max_depth,\n",
    "        \"min_samples_split\": rf_model.min_samples_split,\n",
    "        \"min_samples_leaf\": rf_model.min_samples_leaf\n",
    "    },\n",
    "    \"feature_info\": {\n",
    "        \"original_features\": X_train_raw.shape[1],\n",
    "        \"engineered_features\": X_model.shape[1],\n",
    "        \"feature_names\": X_model.columns.tolist()\n",
    "    },\n",
    "    \"data_sources\": {\n",
    "        \"train_data\": \"data_splits/train_data_splitted.csv\",\n",
    "        \"validation_data\": \"data_splits/validation_data_splitted.csv\",\n",
    "        \"original_data\": \"code/train_data.csv\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save results to JSON\n",
    "baseline_results_path = baseline_dir / f\"baseline_results_{baseline_timestamp}.json\"\n",
    "with open(baseline_results_path, 'w') as f:\n",
    "    json.dump(baseline_results, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Baseline results saved: {baseline_results_path.name}\")\n",
    "\n",
    "# Create baseline summary file\n",
    "baseline_summary = f\"\"\"\n",
    "# BigMart Sales - Baseline Model Summary\n",
    "Generated: {baseline_timestamp}\n",
    "\n",
    "## üìä Baseline Performance\n",
    "- **Cross-Validation R¬≤**: {cv_scores_r2.mean():.4f} ¬± {cv_scores_r2.std():.4f}\n",
    "- **Cross-Validation RMSE**: ${cv_scores_rmse.mean():.2f} ¬± ${cv_scores_rmse.std():.2f}\n",
    "- **Validation R¬≤**: {r2:.4f}\n",
    "- **Validation RMSE**: ${rmse:.2f}\n",
    "\n",
    "## üö® Key Issues Identified\n",
    "- **Overfitting**: Validation R¬≤ is {abs(cv_scores_r2.mean() - r2):.4f} lower than CV\n",
    "- **Poor Generalization**: Model doesn't generalize to unseen items\n",
    "- **Performance Gap**: {((cv_scores_r2.mean() - r2) / cv_scores_r2.mean() * 100):.1f}% performance loss\n",
    "\n",
    "## üìÅ Data Setup\n",
    "- **Training Data**: train_data_splitted.csv ({len(train_data)} records, {train_data['Item_Identifier'].nunique()} items)\n",
    "- **Validation Data**: validation_data_splitted.csv ({len(validation_data)} records, {validation_data['Item_Identifier'].nunique()} items)\n",
    "- **No Data Leakage**: ‚úÖ Confirmed\n",
    "\n",
    "## üéØ Next Steps for Improvement\n",
    "1. **Reduce Item-Specific Overfitting**: Remove/modify item statistics and target encoding\n",
    "2. **Focus on Generalizable Features**: Emphasize features that work for unseen items\n",
    "3. **Regularization**: Add model regularization to prevent overfitting\n",
    "4. **Feature Selection**: Remove features that don't generalize well\n",
    "5. **Alternative Models**: Try models less prone to overfitting\n",
    "\n",
    "## üì¶ Saved Files\n",
    "- Model: {baseline_model_path.name}\n",
    "- Pipeline: {baseline_pipeline_path.name}\n",
    "- Results: {baseline_results_path.name}\n",
    "\"\"\"\n",
    "\n",
    "baseline_summary_path = baseline_dir / f\"baseline_summary_{baseline_timestamp}.md\"\n",
    "with open(baseline_summary_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(baseline_summary)\n",
    "\n",
    "print(f\"‚úÖ Baseline summary saved: {baseline_summary_path.name}\")\n",
    "\n",
    "print(f\"\\nüéØ BASELINE ESTABLISHED!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"üìä Cross-Validation Performance: R¬≤ = {cv_scores_r2.mean():.4f}, RMSE = ${cv_scores_rmse.mean():.2f}\")\n",
    "print(f\"üìä Validation Performance: R¬≤ = {r2:.4f}, RMSE = ${rmse:.2f}\")\n",
    "print(f\"üö® Performance Gap: R¬≤ drops by {abs(cv_scores_r2.mean() - r2):.4f}\")\n",
    "print(f\"\\nüìÅ Standard Data Files:\")\n",
    "print(f\"   ‚Ä¢ Training: data_splits/train_data_splitted.csv\")\n",
    "print(f\"   ‚Ä¢ Validation: data_splits/validation_data_splitted.csv\")\n",
    "print(f\"\\nüöÄ Ready for model improvement iterations!\")\n",
    "print(f\"   ‚Ä¢ Use baseline as comparison point\")\n",
    "print(f\"   ‚Ä¢ Focus on reducing overfitting\")\n",
    "print(f\"   ‚Ä¢ Improve generalization to unseen items\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
