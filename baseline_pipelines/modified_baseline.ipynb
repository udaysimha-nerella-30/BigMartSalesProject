{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5778cd3",
   "metadata": {},
   "source": [
    "# ğŸ”¬ BigMart Sales - Proper Train/Validation Split Approach\n",
    "\n",
    "This notebook implements a robust validation methodology by:\n",
    "\n",
    "1. **Loading original training data**\n",
    "2. **Creating proper train/validation splits** \n",
    "3. **Saving splits separately** (train_splitted.csv, validation_splitted.csv)\n",
    "4. **Training only on train_splitted.csv**\n",
    "5. **Validating on validation_splitted.csv** (unseen data)\n",
    "\n",
    "This approach ensures true validation performance by eliminating any potential data leakage between training and validation phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23d85960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¬ BigMart Sales - Proper Train/Validation Split Approach\n",
      "============================================================\n",
      "âœ… Libraries imported successfully\n",
      "ğŸ² Random state set to: 42\n"
     ]
    }
   ],
   "source": [
    "# 1. Import Libraries and Setup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import cross_val_score, GroupKFold, RandomizedSearchCV, GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import KNNImputer\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random state for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "print(\"ğŸ”¬ BigMart Sales - Proper Train/Validation Split Approach\")\n",
    "print(\"=\" * 60)\n",
    "print(\"âœ… Libraries imported successfully\")\n",
    "print(f\"ğŸ² Random state set to: {RANDOM_STATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbe9da68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š Loading Original Data, Creating Split, and Saving...\n",
      "------------------------------------------------------------\n",
      "âœ… Original training data loaded: (8523, 12)\n",
      "âœ… Data split created:\n",
      "   â€¢ Training split: (6818, 12) (80.0%)\n",
      "   â€¢ Validation split: (1705, 12) (20.0%)\n",
      "   â€¢ Overlap: 0 (should be 0) - âœ… GOOD\n",
      "\n",
      "ğŸ’¾ Files Saved:\n",
      "   â€¢ train_data_splitted.csv\n",
      "   â€¢ validation_data_splitted.csv\n",
      "\n",
      "ğŸ§¹ Memory cleared - ready for clean training process\n"
     ]
    }
   ],
   "source": [
    "# 2. Load Original Data, Create ONE Split, and Save to CSV Files\n",
    "print(\"ğŸ“Š Loading Original Data, Creating Split, and Saving...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Load original training data\n",
    "train_data_raw = pd.read_csv('code/train_data.csv')\n",
    "print(f\"âœ… Original training data loaded: {train_data_raw.shape}\")\n",
    "\n",
    "# Create ONE train/validation split using GroupKFold\n",
    "cv_strategy = GroupKFold(n_splits=5)\n",
    "groups = train_data_raw['Item_Identifier']\n",
    "\n",
    "# Get the first split (80/20 split approximately)\n",
    "train_idx, val_idx = next(cv_strategy.split(train_data_raw, train_data_raw['Item_Outlet_Sales'], groups))\n",
    "\n",
    "# Create train and validation datasets\n",
    "train_data_split = train_data_raw.iloc[train_idx].copy()\n",
    "validation_data_split = train_data_raw.iloc[val_idx].copy()\n",
    "\n",
    "print(f\"âœ… Data split created:\")\n",
    "print(f\"   â€¢ Training split: {train_data_split.shape} ({len(train_idx)/len(train_data_raw)*100:.1f}%)\")\n",
    "print(f\"   â€¢ Validation split: {validation_data_split.shape} ({len(val_idx)/len(train_data_raw)*100:.1f}%)\")\n",
    "\n",
    "# Verify no item overlap\n",
    "train_items = set(train_data_split['Item_Identifier'])\n",
    "val_items = set(validation_data_split['Item_Identifier'])\n",
    "overlap = train_items.intersection(val_items)\n",
    "print(f\"   â€¢ Overlap: {len(overlap)} (should be 0) - {'âœ… GOOD' if len(overlap) == 0 else 'âŒ BAD'}\")\n",
    "\n",
    "# Save splits to CSV files\n",
    "output_dir = Path(\"data_splits\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "train_split_path = output_dir / \"train_data_splitted.csv\"\n",
    "validation_split_path = output_dir / \"validation_data_splitted.csv\"\n",
    "\n",
    "train_data_split.to_csv(train_split_path, index=False)\n",
    "validation_data_split.to_csv(validation_split_path, index=False)\n",
    "\n",
    "print(f\"\\nğŸ’¾ Files Saved:\")\n",
    "print(f\"   â€¢ {train_split_path.name}\")\n",
    "print(f\"   â€¢ {validation_split_path.name}\")\n",
    "\n",
    "# Clear all data from memory\n",
    "del train_data_raw, train_data_split, validation_data_split, train_idx, val_idx, groups, train_items, val_items\n",
    "print(f\"\\nğŸ§¹ Memory cleared - ready for clean training process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c6d9fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… BigMartPreprocessor class defined\n"
     ]
    }
   ],
   "source": [
    "# 3. Define BigMartPreprocessor Class\n",
    "class BigMartPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Complete preprocessing pipeline for BigMart sales data\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.item_stats = None\n",
    "        self.outlet_stats = None\n",
    "        self.item_target_mean = None\n",
    "        self.overall_mean = None\n",
    "        self.outlet_size_mode = {}\n",
    "        self.is_fitted = False\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        if y is not None:\n",
    "            self.overall_mean = y.mean()\n",
    "            X_temp = X.copy()\n",
    "            X_temp['target'] = y\n",
    "            \n",
    "            # Item statistics\n",
    "            self.item_stats = X_temp.groupby('Item_Identifier')['target'].agg([\n",
    "                'mean', 'median', 'std', 'count'\n",
    "            ]).add_prefix('Item_')\n",
    "            \n",
    "            # Outlet statistics  \n",
    "            self.outlet_stats = X_temp.groupby('Outlet_Identifier')['target'].agg([\n",
    "                'mean', 'median', 'std', 'count'\n",
    "            ]).add_prefix('Outlet_')\n",
    "            \n",
    "            # Target encoding\n",
    "            self.item_target_mean = X_temp.groupby('Item_Identifier')['target'].mean().to_dict()\n",
    "        \n",
    "        # Outlet size mode by outlet type\n",
    "        if 'Outlet_Size' in X.columns and X['Outlet_Size'].isnull().any():\n",
    "            self.outlet_size_mode = X.groupby('Outlet_Type')['Outlet_Size'].apply(\n",
    "                lambda x: x.mode().iloc[0] if not x.mode().empty else 'Medium'\n",
    "            ).to_dict()\n",
    "        \n",
    "        self.is_fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_processed = X.copy()\n",
    "        \n",
    "        # 1. Handle missing values\n",
    "        if 'Item_Weight' in X_processed.columns and X_processed['Item_Weight'].isnull().any():\n",
    "            weight_median_by_type = X_processed.groupby('Item_Type')['Item_Weight'].median()\n",
    "            for item_type in X_processed['Item_Type'].unique():\n",
    "                mask = (X_processed['Item_Type'] == item_type) & X_processed['Item_Weight'].isnull()\n",
    "                if mask.any():\n",
    "                    median_val = weight_median_by_type.get(item_type, X_processed['Item_Weight'].median())\n",
    "                    X_processed.loc[mask, 'Item_Weight'] = median_val\n",
    "        \n",
    "        if 'Outlet_Size' in X_processed.columns and X_processed['Outlet_Size'].isnull().any():\n",
    "            for outlet_type, mode_size in self.outlet_size_mode.items():\n",
    "                mask = (X_processed['Outlet_Type'] == outlet_type) & X_processed['Outlet_Size'].isnull()\n",
    "                if mask.any():\n",
    "                    X_processed.loc[mask, 'Outlet_Size'] = mode_size\n",
    "        \n",
    "        # 2. Enhanced Item_Identifier features\n",
    "        X_processed['Item_Category'] = X_processed['Item_Identifier'].str[:2]\n",
    "        item_numeric = X_processed['Item_Identifier'].str[2:]\n",
    "        X_processed['Item_Number'] = pd.to_numeric(item_numeric, errors='coerce').fillna(0).astype(int)\n",
    "        \n",
    "        category_mapping = {'FD': 'Food', 'NC': 'Non-Consumable', 'DR': 'Drinks'}\n",
    "        X_processed['Item_Category_Group'] = X_processed['Item_Category'].map(category_mapping)\n",
    "        \n",
    "        # Target encoding\n",
    "        if self.item_target_mean is not None:\n",
    "            X_processed['Item_Target_Encoded'] = X_processed['Item_Identifier'].map(self.item_target_mean)\n",
    "            X_processed['Item_Target_Encoded'].fillna(self.overall_mean, inplace=True)\n",
    "        \n",
    "        # 3. Add statistics\n",
    "        if self.item_stats is not None:\n",
    "            X_processed = X_processed.merge(self.item_stats, left_on='Item_Identifier', right_index=True, how='left')\n",
    "        if self.outlet_stats is not None:\n",
    "            X_processed = X_processed.merge(self.outlet_stats, left_on='Outlet_Identifier', right_index=True, how='left')\n",
    "        \n",
    "        # 4. Feature engineering\n",
    "        X_processed['Item_MRP_Bin'] = pd.cut(X_processed['Item_MRP'], bins=4, labels=['Low', 'Medium', 'High', 'Premium'])\n",
    "        X_processed['Outlet_Age'] = 2013 - X_processed['Outlet_Establishment_Year']\n",
    "        X_processed['Outlet_Age_Group'] = pd.cut(X_processed['Outlet_Age'], bins=[0, 10, 20, 30], labels=['New', 'Medium', 'Old'])\n",
    "        X_processed['Item_Visibility_Binned'] = pd.cut(X_processed['Item_Visibility'], bins=5, labels=['Very_Low', 'Low', 'Medium', 'High', 'Very_High'])\n",
    "        \n",
    "        food_categories = ['Dairy', 'Soft Drinks', 'Meat', 'Fruits and Vegetables', \n",
    "                          'Household', 'Baking Goods', 'Snack Foods', 'Frozen Foods',\n",
    "                          'Breakfast', 'Health and Hygiene', 'Hard Drinks', 'Canned',\n",
    "                          'Breads', 'Starchy Foods', 'Others', 'Seafood']\n",
    "        X_processed['Item_Type_Category'] = X_processed['Item_Type'].apply(\n",
    "            lambda x: 'Food' if x in food_categories else 'Non-Food'\n",
    "        )\n",
    "        \n",
    "        # 5. Encode categorical variables\n",
    "        categorical_cols = X_processed.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "        id_cols = ['Item_Identifier', 'Outlet_Identifier']\n",
    "        categorical_cols = [col for col in categorical_cols if col not in id_cols]\n",
    "        \n",
    "        if categorical_cols:\n",
    "            X_encoded = pd.get_dummies(X_processed, columns=categorical_cols, drop_first=True)\n",
    "        else:\n",
    "            X_encoded = X_processed\n",
    "        \n",
    "        # Keep Item_Identifier for GroupKFold, remove Outlet_Identifier\n",
    "        feature_cols = [col for col in X_encoded.columns if col != 'Outlet_Identifier']\n",
    "        X_final = X_encoded[feature_cols]\n",
    "        \n",
    "        return X_final\n",
    "\n",
    "print(\"âœ… BigMartPreprocessor class defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cac21e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Loading Training Split and Starting Model Development...\n",
      "============================================================\n",
      "âœ… Training split loaded: (6818, 12)\n",
      "   â€¢ Unique items: 1247\n",
      "   â€¢ Target mean: $2166.09\n",
      "ğŸ“Š Training Data:\n",
      "   â€¢ Features: (6818, 11)\n",
      "   â€¢ Target: (6818,)\n",
      "âœ… Preprocessing pipeline fitted\n",
      "âœ… Training data transformed: (6818, 11) â†’ (6818, 55)\n",
      "ğŸ“Š Model data ready: (6818, 54)\n",
      "\n",
      "ğŸ”€ Cross-Validation Setup (Training Split Only):\n",
      "   â€¢ Strategy: GroupKFold (5 splits)\n",
      "   â€¢ Groups: 1247 unique items\n",
      "   â€¢ Total records: 6818\n",
      "\n",
      "ğŸ¤– Performing Cross-Validation...\n",
      "\n",
      "ğŸ“Š Cross-Validation Results:\n",
      "   â€¢ RÂ² Mean: 0.6962 Â± 0.0133\n",
      "   â€¢ RMSE Mean: $935.43 Â± $27.64\n",
      "\n",
      "ğŸ¯ Training Final Model on Complete Training Split...\n",
      "âœ… Final model trained\n",
      "ğŸ’¾ Model saved: best_model_20250906_160015.pkl\n",
      "ğŸ’¾ Pipeline saved: preprocessor_20250906_160015.pkl\n",
      "\n",
      "ğŸ¯ Training Complete - Ready for Validation Test!\n",
      "   â€¢ Cross-validation RÂ²: 0.6962\n",
      "   â€¢ Cross-validation RMSE: $935.43\n",
      "   â€¢ Next: Load validation split and test final performance\n"
     ]
    }
   ],
   "source": [
    "# 4. Load ONLY Training Split and Perform Complete Model Development\n",
    "print(\"ğŸš€ Loading Training Split and Starting Model Development...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load ONLY the training split\n",
    "train_data = pd.read_csv(\"data_splits/train_data_splitted.csv\")\n",
    "print(f\"âœ… Training split loaded: {train_data.shape}\")\n",
    "print(f\"   â€¢ Unique items: {train_data['Item_Identifier'].nunique()}\")\n",
    "print(f\"   â€¢ Target mean: ${train_data['Item_Outlet_Sales'].mean():.2f}\")\n",
    "\n",
    "# Prepare features and target\n",
    "X_train_raw = train_data.drop('Item_Outlet_Sales', axis=1)\n",
    "y_train = train_data['Item_Outlet_Sales']\n",
    "\n",
    "print(f\"ğŸ“Š Training Data:\")\n",
    "print(f\"   â€¢ Features: {X_train_raw.shape}\")\n",
    "print(f\"   â€¢ Target: {y_train.shape}\")\n",
    "\n",
    "# Fit preprocessing pipeline on training split only\n",
    "preprocessor = BigMartPreprocessor()\n",
    "preprocessor.fit(X_train_raw, y_train)\n",
    "print(\"âœ… Preprocessing pipeline fitted\")\n",
    "\n",
    "# Transform training data\n",
    "X_train_processed = preprocessor.transform(X_train_raw)\n",
    "print(f\"âœ… Training data transformed: {X_train_raw.shape} â†’ {X_train_processed.shape}\")\n",
    "\n",
    "# Prepare data for modeling (remove Item_Identifier)\n",
    "X_model = X_train_processed.drop('Item_Identifier', axis=1) if 'Item_Identifier' in X_train_processed.columns else X_train_processed\n",
    "print(f\"ğŸ“Š Model data ready: {X_model.shape}\")\n",
    "\n",
    "# Setup GroupKFold cross-validation on training split\n",
    "cv_strategy = GroupKFold(n_splits=5)\n",
    "cv_groups = X_train_processed['Item_Identifier'] if 'Item_Identifier' in X_train_processed.columns else X_train_raw['Item_Identifier']\n",
    "\n",
    "print(f\"\\nğŸ”€ Cross-Validation Setup (Training Split Only):\")\n",
    "print(f\"   â€¢ Strategy: GroupKFold (5 splits)\")\n",
    "print(f\"   â€¢ Groups: {cv_groups.nunique()} unique items\")\n",
    "print(f\"   â€¢ Total records: {len(X_model)}\")\n",
    "\n",
    "# Train Random Forest model with cross-validation\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=15,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f\"\\nğŸ¤– Performing Cross-Validation...\")\n",
    "cv_scores_r2 = cross_val_score(rf_model, X_model, y_train, cv=cv_strategy, \n",
    "                               groups=cv_groups, scoring='r2', n_jobs=-1)\n",
    "cv_scores_neg_rmse = cross_val_score(rf_model, X_model, y_train, cv=cv_strategy, \n",
    "                                     groups=cv_groups, scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "cv_scores_rmse = -cv_scores_neg_rmse\n",
    "\n",
    "print(f\"\\nğŸ“Š Cross-Validation Results:\")\n",
    "print(f\"   â€¢ RÂ² Mean: {cv_scores_r2.mean():.4f} Â± {cv_scores_r2.std():.4f}\")\n",
    "print(f\"   â€¢ RMSE Mean: ${cv_scores_rmse.mean():.2f} Â± ${cv_scores_rmse.std():.2f}\")\n",
    "\n",
    "# Train final model on full training split\n",
    "print(f\"\\nğŸ¯ Training Final Model on Complete Training Split...\")\n",
    "rf_model.fit(X_model, y_train)\n",
    "print(\"âœ… Final model trained\")\n",
    "\n",
    "# Save the model and pipeline\n",
    "models_dir = Path(\"trained_models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_path = models_dir / f\"best_model_{timestamp}.pkl\"\n",
    "pipeline_path = models_dir / f\"preprocessor_{timestamp}.pkl\"\n",
    "\n",
    "joblib.dump(rf_model, model_path)\n",
    "joblib.dump(preprocessor, pipeline_path)\n",
    "\n",
    "print(f\"ğŸ’¾ Model saved: {model_path.name}\")\n",
    "print(f\"ğŸ’¾ Pipeline saved: {pipeline_path.name}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Training Complete - Ready for Validation Test!\")\n",
    "print(f\"   â€¢ Cross-validation RÂ²: {cv_scores_r2.mean():.4f}\")\n",
    "print(f\"   â€¢ Cross-validation RMSE: ${cv_scores_rmse.mean():.2f}\")\n",
    "print(f\"   â€¢ Next: Load validation split and test final performance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b62a0b49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ§ª Loading Validation Split and Testing Final Model...\n",
      "============================================================\n",
      "âœ… Validation split loaded: (1705, 12)\n",
      "   â€¢ Unique items: 312\n",
      "ğŸ“Š Validation Data:\n",
      "   â€¢ Features: (1705, 11)\n",
      "   â€¢ Target: (1705,)\n",
      "   â€¢ Target mean: $2242.07\n",
      "\n",
      "ğŸ”§ Applying Preprocessing Pipeline...\n",
      "âœ… Validation data transformed: (1705, 11) â†’ (1705, 55)\n",
      "ğŸ“Š Validation model data: (1705, 54)\n",
      "\n",
      "ğŸ”® Making Predictions...\n",
      "\n",
      "ğŸ“ˆ FINAL VALIDATION RESULTS:\n",
      "==================================================\n",
      "ğŸ“Š RÂ² Score: 0.2088\n",
      "ğŸ’° RMSE: $1535.87\n",
      "ğŸ“Š MAE: $1064.87\n",
      "\n",
      "ğŸ“Š Cross-Validation vs Validation Comparison:\n",
      "   â€¢ CV RÂ²: 0.6962 â†’ Validation RÂ²: 0.2088\n",
      "   â€¢ CV RMSE: $935.43 â†’ Validation RMSE: $1535.87\n",
      "\n",
      "ğŸ¯ Performance Assessment:\n",
      "âš ï¸ CONCERNS: Validation performance below expectations\n",
      "   â€¢ May need model improvement\n",
      "\n",
      "ğŸ‰ VALIDATION COMPLETE!\n",
      "   â€¢ True validation on completely unseen data\n",
      "   â€¢ No data leakage between training and validation\n",
      "   â€¢ Robust performance assessment\n"
     ]
    }
   ],
   "source": [
    "# 5. Load Validation Split and Test Final Model Performance\n",
    "print(\"ğŸ§ª Loading Validation Split and Testing Final Model...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load validation split (completely unseen data)\n",
    "validation_data = pd.read_csv(\"data_splits/validation_data_splitted.csv\")\n",
    "print(f\"âœ… Validation split loaded: {validation_data.shape}\")\n",
    "print(f\"   â€¢ Unique items: {validation_data['Item_Identifier'].nunique()}\")\n",
    "\n",
    "# Prepare validation features and target\n",
    "X_val_raw = validation_data.drop('Item_Outlet_Sales', axis=1)\n",
    "y_val = validation_data['Item_Outlet_Sales']\n",
    "\n",
    "print(f\"ğŸ“Š Validation Data:\")\n",
    "print(f\"   â€¢ Features: {X_val_raw.shape}\")\n",
    "print(f\"   â€¢ Target: {y_val.shape}\")\n",
    "print(f\"   â€¢ Target mean: ${y_val.mean():.2f}\")\n",
    "\n",
    "# Apply preprocessing pipeline (fitted on training data only)\n",
    "print(f\"\\nğŸ”§ Applying Preprocessing Pipeline...\")\n",
    "X_val_processed = preprocessor.transform(X_val_raw)\n",
    "print(f\"âœ… Validation data transformed: {X_val_raw.shape} â†’ {X_val_processed.shape}\")\n",
    "\n",
    "# Prepare for model prediction (remove Item_Identifier)\n",
    "X_val_model = X_val_processed.drop('Item_Identifier', axis=1) if 'Item_Identifier' in X_val_processed.columns else X_val_processed\n",
    "print(f\"ğŸ“Š Validation model data: {X_val_model.shape}\")\n",
    "\n",
    "# Make predictions\n",
    "print(f\"\\nğŸ”® Making Predictions...\")\n",
    "y_pred = rf_model.predict(X_val_model)\n",
    "\n",
    "# Calculate metrics\n",
    "r2 = r2_score(y_val, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "\n",
    "print(f\"\\nğŸ“ˆ FINAL VALIDATION RESULTS:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ğŸ“Š RÂ² Score: {r2:.4f}\")\n",
    "print(f\"ğŸ’° RMSE: ${rmse:.2f}\")\n",
    "print(f\"ğŸ“Š MAE: ${mae:.2f}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Cross-Validation vs Validation Comparison:\")\n",
    "print(f\"   â€¢ CV RÂ²: {cv_scores_r2.mean():.4f} â†’ Validation RÂ²: {r2:.4f}\")\n",
    "print(f\"   â€¢ CV RMSE: ${cv_scores_rmse.mean():.2f} â†’ Validation RMSE: ${rmse:.2f}\")\n",
    "\n",
    "# Performance assessment\n",
    "print(f\"\\nğŸ¯ Performance Assessment:\")\n",
    "if abs(r2 - cv_scores_r2.mean()) <= 0.05 and abs(rmse - cv_scores_rmse.mean()) <= 100:\n",
    "    print(\"âœ… EXCELLENT! Validation performance matches cross-validation\")\n",
    "    print(\"   â€¢ No overfitting detected\")\n",
    "    print(\"   â€¢ Model generalizes well to unseen data\")\n",
    "elif r2 >= 0.65 and rmse <= 1000:\n",
    "    print(\"âœ… GOOD! Validation performance is acceptable\")\n",
    "    print(\"   â€¢ Meets performance thresholds\")\n",
    "else:\n",
    "    print(\"âš ï¸ CONCERNS: Validation performance below expectations\")\n",
    "    print(\"   â€¢ May need model improvement\")\n",
    "\n",
    "print(f\"\\nğŸ‰ VALIDATION COMPLETE!\")\n",
    "print(f\"   â€¢ True validation on completely unseen data\")\n",
    "print(f\"   â€¢ No data leakage between training and validation\")\n",
    "print(f\"   â€¢ Robust performance assessment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba935fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Saving Baseline Models and Results...\n",
      "============================================================\n",
      "âœ… Baseline model saved: baseline_model_20250906_160544.pkl\n",
      "âœ… Baseline pipeline saved: baseline_preprocessor_20250906_160544.pkl\n",
      "âœ… Baseline results saved: baseline_results_20250906_160544.json\n",
      "âœ… Baseline summary saved: baseline_summary_20250906_160544.md\n",
      "\n",
      "ğŸ¯ BASELINE ESTABLISHED!\n",
      "==================================================\n",
      "ğŸ“Š Cross-Validation Performance: RÂ² = 0.6962, RMSE = $935.43\n",
      "ğŸ“Š Validation Performance: RÂ² = 0.2088, RMSE = $1535.87\n",
      "ğŸš¨ Performance Gap: RÂ² drops by 0.4874\n",
      "\n",
      "ğŸ“ Standard Data Files:\n",
      "   â€¢ Training: data_splits/train_data_splitted.csv\n",
      "   â€¢ Validation: data_splits/validation_data_splitted.csv\n",
      "\n",
      "ğŸš€ Ready for model improvement iterations!\n",
      "   â€¢ Use baseline as comparison point\n",
      "   â€¢ Focus on reducing overfitting\n",
      "   â€¢ Improve generalization to unseen items\n"
     ]
    }
   ],
   "source": [
    "# 6. Save Baseline Models and Results for Future Improvement\n",
    "print(\"ğŸ’¾ Saving Baseline Models and Results...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create baseline directory\n",
    "baseline_dir = Path(\"baseline_models\")\n",
    "baseline_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Generate timestamp for baseline\n",
    "baseline_timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# Save models with baseline naming\n",
    "baseline_model_path = baseline_dir / f\"baseline_model_{baseline_timestamp}.pkl\"\n",
    "baseline_pipeline_path = baseline_dir / f\"baseline_preprocessor_{baseline_timestamp}.pkl\"\n",
    "\n",
    "joblib.dump(rf_model, baseline_model_path)\n",
    "joblib.dump(preprocessor, baseline_pipeline_path)\n",
    "\n",
    "print(f\"âœ… Baseline model saved: {baseline_model_path.name}\")\n",
    "print(f\"âœ… Baseline pipeline saved: {baseline_pipeline_path.name}\")\n",
    "\n",
    "# Save baseline performance results\n",
    "baseline_results = {\n",
    "    \"timestamp\": baseline_timestamp,\n",
    "    \"model_type\": \"RandomForestRegressor\",\n",
    "    \"random_state\": RANDOM_STATE,\n",
    "    \"data_split\": {\n",
    "        \"method\": \"GroupKFold\",\n",
    "        \"train_records\": len(train_data),\n",
    "        \"validation_records\": len(validation_data),\n",
    "        \"train_items\": train_data['Item_Identifier'].nunique(),\n",
    "        \"validation_items\": validation_data['Item_Identifier'].nunique()\n",
    "    },\n",
    "    \"cross_validation\": {\n",
    "        \"cv_strategy\": \"GroupKFold_5_splits\",\n",
    "        \"r2_mean\": float(cv_scores_r2.mean()),\n",
    "        \"r2_std\": float(cv_scores_r2.std()),\n",
    "        \"rmse_mean\": float(cv_scores_rmse.mean()),\n",
    "        \"rmse_std\": float(cv_scores_rmse.std()),\n",
    "        \"individual_folds\": {\n",
    "            \"r2_scores\": cv_scores_r2.tolist(),\n",
    "            \"rmse_scores\": cv_scores_rmse.tolist()\n",
    "        }\n",
    "    },\n",
    "    \"validation_performance\": {\n",
    "        \"r2_score\": float(r2),\n",
    "        \"rmse\": float(rmse),\n",
    "        \"mae\": float(mae),\n",
    "        \"target_mean_train\": float(train_data['Item_Outlet_Sales'].mean()),\n",
    "        \"target_mean_validation\": float(validation_data['Item_Outlet_Sales'].mean())\n",
    "    },\n",
    "    \"performance_gap\": {\n",
    "        \"r2_gap\": float(cv_scores_r2.mean() - r2),\n",
    "        \"rmse_gap\": float(rmse - cv_scores_rmse.mean()),\n",
    "        \"overfitting_detected\": bool(abs(cv_scores_r2.mean() - r2) > 0.05)\n",
    "    },\n",
    "    \"model_parameters\": {\n",
    "        \"n_estimators\": rf_model.n_estimators,\n",
    "        \"max_depth\": rf_model.max_depth,\n",
    "        \"min_samples_split\": rf_model.min_samples_split,\n",
    "        \"min_samples_leaf\": rf_model.min_samples_leaf\n",
    "    },\n",
    "    \"feature_info\": {\n",
    "        \"original_features\": X_train_raw.shape[1],\n",
    "        \"engineered_features\": X_model.shape[1],\n",
    "        \"feature_names\": X_model.columns.tolist()\n",
    "    },\n",
    "    \"data_sources\": {\n",
    "        \"train_data\": \"data_splits/train_data_splitted.csv\",\n",
    "        \"validation_data\": \"data_splits/validation_data_splitted.csv\",\n",
    "        \"original_data\": \"code/train_data.csv\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save results to JSON\n",
    "baseline_results_path = baseline_dir / f\"baseline_results_{baseline_timestamp}.json\"\n",
    "with open(baseline_results_path, 'w') as f:\n",
    "    json.dump(baseline_results, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Baseline results saved: {baseline_results_path.name}\")\n",
    "\n",
    "# Create baseline summary file\n",
    "baseline_summary = f\"\"\"\n",
    "# BigMart Sales - Baseline Model Summary\n",
    "Generated: {baseline_timestamp}\n",
    "\n",
    "## ğŸ“Š Baseline Performance\n",
    "- **Cross-Validation RÂ²**: {cv_scores_r2.mean():.4f} Â± {cv_scores_r2.std():.4f}\n",
    "- **Cross-Validation RMSE**: ${cv_scores_rmse.mean():.2f} Â± ${cv_scores_rmse.std():.2f}\n",
    "- **Validation RÂ²**: {r2:.4f}\n",
    "- **Validation RMSE**: ${rmse:.2f}\n",
    "\n",
    "## ğŸš¨ Key Issues Identified\n",
    "- **Overfitting**: Validation RÂ² is {abs(cv_scores_r2.mean() - r2):.4f} lower than CV\n",
    "- **Poor Generalization**: Model doesn't generalize to unseen items\n",
    "- **Performance Gap**: {((cv_scores_r2.mean() - r2) / cv_scores_r2.mean() * 100):.1f}% performance loss\n",
    "\n",
    "## ğŸ“ Data Setup\n",
    "- **Training Data**: train_data_splitted.csv ({len(train_data)} records, {train_data['Item_Identifier'].nunique()} items)\n",
    "- **Validation Data**: validation_data_splitted.csv ({len(validation_data)} records, {validation_data['Item_Identifier'].nunique()} items)\n",
    "- **No Data Leakage**: âœ… Confirmed\n",
    "\n",
    "## ğŸ¯ Next Steps for Improvement\n",
    "1. **Reduce Item-Specific Overfitting**: Remove/modify item statistics and target encoding\n",
    "2. **Focus on Generalizable Features**: Emphasize features that work for unseen items\n",
    "3. **Regularization**: Add model regularization to prevent overfitting\n",
    "4. **Feature Selection**: Remove features that don't generalize well\n",
    "5. **Alternative Models**: Try models less prone to overfitting\n",
    "\n",
    "## ğŸ“¦ Saved Files\n",
    "- Model: {baseline_model_path.name}\n",
    "- Pipeline: {baseline_pipeline_path.name}\n",
    "- Results: {baseline_results_path.name}\n",
    "\"\"\"\n",
    "\n",
    "baseline_summary_path = baseline_dir / f\"baseline_summary_{baseline_timestamp}.md\"\n",
    "with open(baseline_summary_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(baseline_summary)\n",
    "\n",
    "print(f\"âœ… Baseline summary saved: {baseline_summary_path.name}\")\n",
    "\n",
    "print(f\"\\nğŸ¯ BASELINE ESTABLISHED!\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"ğŸ“Š Cross-Validation Performance: RÂ² = {cv_scores_r2.mean():.4f}, RMSE = ${cv_scores_rmse.mean():.2f}\")\n",
    "print(f\"ğŸ“Š Validation Performance: RÂ² = {r2:.4f}, RMSE = ${rmse:.2f}\")\n",
    "print(f\"ğŸš¨ Performance Gap: RÂ² drops by {abs(cv_scores_r2.mean() - r2):.4f}\")\n",
    "print(f\"\\nğŸ“ Standard Data Files:\")\n",
    "print(f\"   â€¢ Training: data_splits/train_data_splitted.csv\")\n",
    "print(f\"   â€¢ Validation: data_splits/validation_data_splitted.csv\")\n",
    "print(f\"\\nğŸš€ Ready for model improvement iterations!\")\n",
    "print(f\"   â€¢ Use baseline as comparison point\")\n",
    "print(f\"   â€¢ Focus on reducing overfitting\")\n",
    "print(f\"   â€¢ Improve generalization to unseen items\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
